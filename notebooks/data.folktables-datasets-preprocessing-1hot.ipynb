{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4d0fb85",
   "metadata": {},
   "source": [
    "# Generate folktables datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f222039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from folktables import ACSDataSource, ACSIncome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "811ad844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important constants!\n",
    "TRAIN_SIZE = 0.7\n",
    "TEST_SIZE = 0.3\n",
    "VALIDATION_SIZE = None\n",
    "\"\"\"\n",
    "TRAIN_SIZE = 0.6\n",
    "TEST_SIZE = 0.2\n",
    "VALIDATION_SIZE = 0.2\n",
    "\"\"\"\n",
    "\n",
    "# MAX_SENSITIVE_GROUPS = 4        # discard samples from small sensitive groups, keep only this many\n",
    "MAX_SENSITIVE_GROUPS = 2\n",
    "# MAX_SENSITIVE_GROUPS = None\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "377fc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert TRAIN_SIZE + TEST_SIZE + (VALIDATION_SIZE or 0.) == 1  # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60b5f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data dir\n",
    "root_dir = Path(\"~\").expanduser()\n",
    "data_dir = root_dir / \"data\" / \"folktables\"\n",
    "data_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae28e462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download 2018 ACS data\n",
    "from folktables.load_acs import state_list\n",
    "\n",
    "data_source = ACSDataSource(\n",
    "    survey_year='2018', horizon='1-Year', survey='person',\n",
    "    root_dir=str(data_dir),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8afe4020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3236107, 286)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data is 3236107 rows x 286 columns\n",
    "acs_data = data_source.get_data(states=state_list, download=True)  # use download=True if not yet downloaded\n",
    "acs_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea8d039",
   "metadata": {},
   "source": [
    "According to the dataset's datasheet, train/test splits should be stratified by state\n",
    "(at least for ACSIncome, the remaining tasks seem ambiguous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4e23b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_COL = \"ST\"\n",
    "\n",
    "ACS_CATEGORICAL_COLS = {\n",
    "    'COW',  # class of worker\n",
    "    'MAR',  # marital status\n",
    "    'OCCP', # occupation code\n",
    "    'POBP', # place of birth code\n",
    "    'RELP', # relationship status\n",
    "    'SEX',\n",
    "    'RAC1P', # race code\n",
    "    'DIS',  # disability\n",
    "    'ESP',  # employment status of parents\n",
    "    'CIT',  # citizenship status\n",
    "    'MIG',  # mobility status\n",
    "    'MIL',  # military service\n",
    "    'ANC',  # ancestry\n",
    "    'NATIVITY',\n",
    "    'DEAR',\n",
    "    'DEYE',\n",
    "    'DREM',\n",
    "    'ESR',\n",
    "    'ST',\n",
    "    'FER',\n",
    "    'GCL',\n",
    "    'JWTR',\n",
    "#     'PUMA',\n",
    "#     'POWPUMA',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37a6792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "from functools import reduce\n",
    "from operator import or_\n",
    "\n",
    "import folktables\n",
    "from folktables import BasicProblem\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_folktables_task(\n",
    "        acs_data: pd.DataFrame,\n",
    "        acs_task_name: str,\n",
    "        train_size: float,\n",
    "        test_size: float,\n",
    "        validation_size: float = None,\n",
    "        max_sensitive_groups: int = None,\n",
    "        stratify_by_state: bool = True,\n",
    "        save_to_dir: Path = None,\n",
    "        seed: int = 42,\n",
    "    ) -> Tuple[pd.DataFrame, ...]:\n",
    "    \"\"\"Train/test split a given folktables task (or train/test/validation).\n",
    "    \n",
    "    According to the dataset's datasheet, (at least) the ACSIncome\n",
    "    task should be stratified by state.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (train_data, test_data, validation_data) : Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n",
    "    \"\"\"\n",
    "    # Sanity check\n",
    "    assert train_size + test_size + (validation_size or 0.0) == 1\n",
    "    assert all(val is None or 0 <= val <= 1 for val in (train_size, test_size, validation_size))\n",
    "    \n",
    "    # Dynamically import/load task object\n",
    "    acs_task = getattr(folktables, acs_task_name)\n",
    "\n",
    "    # Add State to the feature columns so we can do stratified splits (will be removed later)\n",
    "    remove_state_col_later = False # only remove the state column later if we were the ones adding it\n",
    "    if stratify_by_state:\n",
    "        if STATE_COL not in acs_task.features:\n",
    "            acs_task = deepcopy(acs_task) # we're gonna need to change this task object\n",
    "            acs_task.features.append(STATE_COL)\n",
    "            remove_state_col_later = True\n",
    "        else:\n",
    "            remove_state_col_later = False\n",
    "\n",
    "    # Pre-process data + select task-specific features\n",
    "    features, label, group = acs_task.df_to_numpy(acs_data)\n",
    "\n",
    "    # Make a DataFrame with all processed data\n",
    "    df = pd.DataFrame(data=features, columns=acs_task.features)\n",
    "    df[acs_task.target] = label\n",
    "\n",
    "    # Correct column ordering (1st: label, 2nd: group, 3rd and onwards: features)\n",
    "    cols_order = ([acs_task.target, acs_task.group] +\n",
    "        list(set(acs_task.features) - {acs_task.group}))\n",
    "    if remove_state_col_later:\n",
    "        cols_order = [col for col in cols_order if col != STATE_COL]\n",
    "\n",
    "    # Save state_col for stratified split\n",
    "    if stratify_by_state:\n",
    "        state_col_data = df[STATE_COL]\n",
    "\n",
    "    # Enforce correct ordering in df\n",
    "    df = df[cols_order]\n",
    "\n",
    "    # Drop samples from sensitive groups with low relative size\n",
    "    # (e.g., original paper has only White and Black races)\n",
    "    if max_sensitive_groups is not None and max_sensitive_groups > 0:\n",
    "        group_sizes = df.value_counts(acs_task.group, sort=True, ascending=False)\n",
    "        big_groups = group_sizes.index.to_list()[: max_sensitive_groups]\n",
    "\n",
    "        big_groups_filter = reduce(\n",
    "            or_,\n",
    "            [(df[acs_task.group].to_numpy() == g) for g in big_groups],\n",
    "        )\n",
    "        \n",
    "        # Keep only big groups\n",
    "        df = df[big_groups_filter]\n",
    "        state_col_data = state_col_data[big_groups_filter]\n",
    "        \n",
    "        # Group values must be sorted, and start at 0\n",
    "        # (e.g., if we deleted group=2 but kept group=3, the later should now have value 2)\n",
    "        if df[acs_task.group].max() > df[acs_task.group].nunique():\n",
    "            map_to_sequential = {g: idx for g, idx in zip(big_groups, range(len(big_groups)))}\n",
    "            df[acs_task.group] = [map_to_sequential[g] for g in df[acs_task.group]]\n",
    "\n",
    "            logging.warning(f\"Using the following group value mapping: {map_to_sequential}\")\n",
    "            assert df[acs_task.group].max() == df[acs_task.group].nunique() - 1\n",
    "\n",
    "    ## Try to enforce correct types\n",
    "    # All columns should be encoded as integers, dtype=int\n",
    "    types_dict = {\n",
    "        col: int for col in df.columns\n",
    "        if df.dtypes[col] != \"object\"\n",
    "    }\n",
    "    \n",
    "    df = df.astype(types_dict)\n",
    "    # ^ set int types right-away so that categories don't have floating points\n",
    "    \n",
    "    # Set categorical columns to start at value=0! (necessary for sensitive attributes)\n",
    "    for col in (ACS_CATEGORICAL_COLS & set(df.columns)):\n",
    "        df[col] = df[col] - df[col].min()\n",
    "\n",
    "    # Set categorical columns to the correct dtype \"category\"\n",
    "    types_dict.update({\n",
    "        col: \"category\" for col in (ACS_CATEGORICAL_COLS & set(df.columns))\n",
    "        # if df[col].nunique() < 10\n",
    "    })\n",
    "\n",
    "    # Plus the group is definitely categorical\n",
    "    types_dict.update({acs_task.group: \"category\"})\n",
    "    \n",
    "    # And the target is definitely integer\n",
    "    types_dict.update({acs_task.target: int})\n",
    "    \n",
    "    # Set df to correct types\n",
    "    df = df.astype(types_dict)\n",
    "\n",
    "    # ** Split data in train/test/validation **\n",
    "    train_idx, other_idx = train_test_split(\n",
    "        df.index,\n",
    "        train_size=train_size,\n",
    "        stratify=state_col_data if stratify_by_state else None,\n",
    "        random_state=seed,\n",
    "        shuffle=True)\n",
    "\n",
    "    train_df, other_df = df.loc[train_idx], df.loc[other_idx]\n",
    "    assert len(set(train_idx) & set(other_idx)) == 0\n",
    "\n",
    "    # Split validation\n",
    "    if validation_size is not None and validation_size > 0:\n",
    "        new_test_size = test_size / (test_size + validation_size)\n",
    "\n",
    "        val_idx, test_idx = train_test_split(\n",
    "            other_df.index,\n",
    "            test_size=new_test_size,\n",
    "            stratify=state_col_data.loc[other_idx] if stratify_by_state else None,\n",
    "            random_state=seed,\n",
    "            shuffle=True)\n",
    "\n",
    "        val_df, test_df = other_df.loc[val_idx], other_df.loc[test_idx]\n",
    "        assert len(train_idx) + len(val_idx) + len(test_idx) == len(df)\n",
    "        assert np.isclose(len(val_df) / len(df), validation_size)\n",
    "\n",
    "    else:\n",
    "        test_idx = other_idx\n",
    "        test_df = other_df\n",
    "\n",
    "    assert np.isclose(len(train_df) / len(df), train_size)\n",
    "    assert np.isclose(len(test_df) / len(df), test_size)\n",
    "    \n",
    "    # Optionally, save data to disk\n",
    "    if save_to_dir:\n",
    "        print(f\"Saving data to folder '{str(save_to_dir)}' with prefix '{acs_task_name}'.\")\n",
    "        train_df.to_csv(save_to_dir / f\"{acs_task_name}.train.csv\", header=True, index_label=\"index\")\n",
    "        test_df.to_csv(save_to_dir / f\"{acs_task_name}.test.csv\", header=True, index_label=\"index\")\n",
    "        \n",
    "        if validation_size:\n",
    "            val_df.to_csv(save_to_dir / f\"{acs_task_name}.validation.csv\", header=True, index_label=\"index\")\n",
    "\n",
    "    return (train_df, test_df, val_df) if validation_size else (train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a7d5a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "\n",
    "def save_pickle(obj: object, path: str | Path, overwrite: bool = True) -> bool:\n",
    "    \"\"\"Saves the given object as a pickle with the given file path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : object\n",
    "        The object to pickle\n",
    "    path : str or Path\n",
    "        The file path to save the pickle with.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    success : bool\n",
    "        Whether pickling was successful.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path, \"wb\" if overwrite else \"xb\") as f_out:\n",
    "            cloudpickle.dump(obj, f_out)\n",
    "            return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Pickling failed with exception '{e}'\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd4f8f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from pandas.api.types import is_categorical_dtype, is_numeric_dtype\n",
    "\n",
    "def onehot_encode_data(\n",
    "        acs_task_name: str,\n",
    "        train_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        val_df: pd.DataFrame = None,\n",
    "        save_to_dir: Path = None,\n",
    "    ) -> tuple[pd.DataFrame]:\n",
    "    \"\"\"Preprocesses the given data for NNs and other 'numeric-only' algorithms, \n",
    "    including: one-hot encoding categorical data, and scaling numeric data to \n",
    "    zero mean and unit stddev.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dynamically import/load task object\n",
    "    task_obj = getattr(folktables, acs_task_name)\n",
    "    \n",
    "    # Split DF in categorical data and numeric data\n",
    "    cat_cols = [col for col in train_df.columns if is_categorical_dtype(train_df[col])]\n",
    "    numeric_cols = [col for col in train_df.columns if col not in cat_cols and col != task_obj.target]\n",
    "\n",
    "    # Fit 1-hot encoder only to the categorical data\n",
    "    enc = OneHotEncoder(\n",
    "        # drop=\"first\",\n",
    "        drop=\"if_binary\",\n",
    "        sparse=False,\n",
    "        min_frequency=0.005,\n",
    "        handle_unknown=\"infrequent_if_exist\")\n",
    "    enc.fit(train_df[cat_cols])\n",
    "    \n",
    "    # Fit standard scaler to numeric data (not the labels though!)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_df[numeric_cols])\n",
    "    \n",
    "    # Save encoder and scaler pickles to disk\n",
    "    save_pickle(enc, save_to_dir / f\"{acs_task_name}.one-hot-encoder.pkl\")\n",
    "    save_pickle(scaler, save_to_dir / f\"{acs_task_name}.standard-scaler.pkl\")\n",
    "\n",
    "    def onehot_helper(df):\n",
    "        \"\"\"Helper function to transform and preprocess the given DF.\"\"\"\n",
    "\n",
    "        # Encode categorical data\n",
    "        df_cat_1hot = pd.DataFrame(\n",
    "            data=enc.transform(df[cat_cols]),\n",
    "            columns=enc.get_feature_names_out(cat_cols),\n",
    "            index=df.index,\n",
    "        )\n",
    "        \n",
    "        # Standardize numerical data\n",
    "        df_numeric_standardized = pd.DataFrame(\n",
    "            data=scaler.transform(df[numeric_cols]),\n",
    "            columns=numeric_cols,\n",
    "            index=df.index,\n",
    "        )\n",
    "\n",
    "        # Concatenate encoded categorical data with standardized numerical data\n",
    "        df_processed = pd.concat(\n",
    "            (\n",
    "                df[task_obj.target],\n",
    "                df[task_obj.group],\n",
    "                df_numeric_standardized,\n",
    "                df_cat_1hot\n",
    "            ), axis=1, ignore_index=False, join=\"inner\")\n",
    "\n",
    "        assert len(df) == len(df_processed)\n",
    "        return df_processed\n",
    "\n",
    "    # For each data split: encode categorical data and do some extra data processing\n",
    "    train_df_1hot = onehot_helper(train_df)\n",
    "    test_df_1hot = onehot_helper(test_df)\n",
    "    val_df_1hot = onehot_helper(val_df) if val_df is not None else None\n",
    "\n",
    "    # Optionally, save data to disk\n",
    "    if save_to_dir:\n",
    "        print(f\"Saving ** 1-hot ** data to folder '{str(save_to_dir)}' with prefix '{acs_task_name}'.\")\n",
    "        train_df_1hot.to_csv(save_to_dir / f\"{acs_task_name}.train.1-hot.csv\", header=True, index_label=\"index\")\n",
    "        test_df_1hot.to_csv(save_to_dir / f\"{acs_task_name}.test.1-hot.csv\", header=True, index_label=\"index\")\n",
    "        \n",
    "        if val_df is not None:\n",
    "            val_df_1hot.to_csv(save_to_dir / f\"{acs_task_name}.validation.1-hot.csv\", header=True, index_label=\"index\")\n",
    "\n",
    "    return (train_df_1hot, test_df_1hot, val_df_1hot) if val_df is not None else (train_df_1hot, test_df_1hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d466b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "all_tasks_max_const_acc = defaultdict(dict)\n",
    "\n",
    "def get_prevalence_details(task_name: str, data: pd.Series, data_type: str) -> str:\n",
    "    label_col = getattr(folktables, task_name).target\n",
    "    label_data = data[label_col]\n",
    "\n",
    "    mode = label_data.mode().values[0]\n",
    "    prev = label_data.mean()\n",
    "    const_acc = max(prev, 1-prev)\n",
    "    \n",
    "    global all_tasks_max_const_acc\n",
    "    all_tasks_max_const_acc[task_name][data_type] = const_acc\n",
    "\n",
    "    return f\"const. accuracy: {const_acc:.2%} \\t (prediction={mode})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d0b1d1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e4546c2d274b639ab36c8947897d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to folder '/Users/acruz/data/folktables/train=0.7_test=0.3_max-groups=2' with prefix 'ACSIncome'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/acruz/opt/miniconda3/envs/py3.10-t5.6/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:808: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/acruz/opt/miniconda3/envs/py3.10-t5.6/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:182: UserWarning: Found unknown categories in columns [6] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ** 1-hot ** data to folder '/Users/acruz/data/folktables/train=0.7_test=0.3_max-groups=2' with prefix 'ACSIncome'.\n",
      "\n",
      "** ACSIncome **\n",
      "\n",
      "train:\tconst. accuracy: 62.41% \t (prediction=0)\n",
      "test:\tconst. accuracy: 62.45% \t (prediction=0)\n",
      "\n",
      "one-hot-encoded version:\n",
      "train:\tconst. accuracy: 62.41% \t (prediction=0)\n",
      "test:\tconst. accuracy: 62.45% \t (prediction=0)\n",
      "Saving data to folder '/Users/acruz/data/folktables/train=0.7_test=0.3_max-groups=2' with prefix 'ACSPublicCoverage'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/acruz/opt/miniconda3/envs/py3.10-t5.6/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:808: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ** 1-hot ** data to folder '/Users/acruz/data/folktables/train=0.7_test=0.3_max-groups=2' with prefix 'ACSPublicCoverage'.\n",
      "\n",
      "** ACSPublicCoverage **\n",
      "\n",
      "train:\tconst. accuracy: 70.57% \t (prediction=0)\n",
      "test:\tconst. accuracy: 70.59% \t (prediction=0)\n",
      "\n",
      "one-hot-encoded version:\n",
      "train:\tconst. accuracy: 70.57% \t (prediction=0)\n",
      "test:\tconst. accuracy: 70.59% \t (prediction=0)\n",
      "Saving data to folder '/Users/acruz/data/folktables/train=0.7_test=0.3_max-groups=2' with prefix 'ACSMobility'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/acruz/opt/miniconda3/envs/py3.10-t5.6/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:808: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ** 1-hot ** data to folder '/Users/acruz/data/folktables/train=0.7_test=0.3_max-groups=2' with prefix 'ACSMobility'.\n",
      "\n",
      "** ACSMobility **\n",
      "\n",
      "train:\tconst. accuracy: 73.60% \t (prediction=1)\n",
      "test:\tconst. accuracy: 73.41% \t (prediction=1)\n",
      "\n",
      "one-hot-encoded version:\n",
      "train:\tconst. accuracy: 73.60% \t (prediction=1)\n",
      "test:\tconst. accuracy: 73.41% \t (prediction=1)\n",
      "Saving data to folder '/Users/acruz/data/folktables/train=0.7_test=0.3_max-groups=2' with prefix 'ACSEmployment'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/acruz/opt/miniconda3/envs/py3.10-t5.6/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:808: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ** 1-hot ** data to folder '/Users/acruz/data/folktables/train=0.7_test=0.3_max-groups=2' with prefix 'ACSEmployment'.\n",
      "\n",
      "** ACSEmployment **\n",
      "\n",
      "train:\tconst. accuracy: 54.28% \t (prediction=0)\n",
      "test:\tconst. accuracy: 54.24% \t (prediction=0)\n",
      "\n",
      "one-hot-encoded version:\n",
      "train:\tconst. accuracy: 54.28% \t (prediction=0)\n",
      "test:\tconst. accuracy: 54.24% \t (prediction=0)\n",
      "Saving data to folder '/Users/acruz/data/folktables/train=0.7_test=0.3_max-groups=2' with prefix 'ACSTravelTime'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/acruz/opt/miniconda3/envs/py3.10-t5.6/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:808: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ** 1-hot ** data to folder '/Users/acruz/data/folktables/train=0.7_test=0.3_max-groups=2' with prefix 'ACSTravelTime'.\n",
      "\n",
      "** ACSTravelTime **\n",
      "\n",
      "train:\tconst. accuracy: 56.97% \t (prediction=0)\n",
      "test:\tconst. accuracy: 57.09% \t (prediction=0)\n",
      "\n",
      "one-hot-encoded version:\n",
      "train:\tconst. accuracy: 56.97% \t (prediction=0)\n",
      "test:\tconst. accuracy: 57.09% \t (prediction=0)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "all_acs_tasks = [\n",
    "    'ACSIncome',\n",
    "    'ACSPublicCoverage',\n",
    "    'ACSMobility',\n",
    "    'ACSEmployment',\n",
    "    'ACSTravelTime',\n",
    "]\n",
    "\n",
    "# Create sub-folder to save data to\n",
    "subfolder_name = f\"train={TRAIN_SIZE:.2}_test={TEST_SIZE:.2}\"\n",
    "if VALIDATION_SIZE:\n",
    "    subfolder_name = f\"{subfolder_name}_validation={VALIDATION_SIZE:.2}\"\n",
    "if MAX_SENSITIVE_GROUPS is not None and MAX_SENSITIVE_GROUPS > 0:\n",
    "    subfolder_name = f\"{subfolder_name}_max-groups={MAX_SENSITIVE_GROUPS}\"\n",
    "\n",
    "subfolder_path = data_dir / subfolder_name\n",
    "subfolder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate data and save to disk, for all tasks\n",
    "for task_name in tqdm(all_acs_tasks):\n",
    "\n",
    "    # Process data\n",
    "    data = split_folktables_task(\n",
    "        acs_data,\n",
    "        acs_task_name=task_name,\n",
    "        train_size=TRAIN_SIZE,\n",
    "        test_size=TEST_SIZE,\n",
    "        validation_size=VALIDATION_SIZE,\n",
    "        max_sensitive_groups=MAX_SENSITIVE_GROUPS,\n",
    "        stratify_by_state=True,\n",
    "        seed=SEED,\n",
    "        save_to_dir=subfolder_path,\n",
    "    )\n",
    "    \n",
    "    # Process 1-hot encodings\n",
    "    data_1hot = onehot_encode_data(\n",
    "        task_name,\n",
    "        *data,\n",
    "        save_to_dir=subfolder_path,\n",
    "    )\n",
    "    \n",
    "    # Print prevalence for each dataset (train, validation, and test)\n",
    "    data_types = (\"train\", \"test\", \"validation\")\n",
    "    print(f\"\\n** {task_name} **\\n\")\n",
    "    print(\"\\n\".join(f\"{data_type}:\\t{get_prevalence_details(task_name, df, data_type)}\" for data_type, df in zip(data_types, data)))\n",
    "    \n",
    "    # Sanity check: prevalence for one-hot version should be equal to that of categorical version\n",
    "    print(\"\\none-hot-encoded version:\")\n",
    "    print(\"\\n\".join(f\"{data_type}:\\t{get_prevalence_details(task_name, df, data_type)}\" for data_type, df in zip(data_types, data_1hot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c40cc10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ACSIncome\": {\n",
      "    \"train\": 0.6240710126295839,\n",
      "    \"test\": 0.6244771852159277\n",
      "  },\n",
      "  \"ACSPublicCoverage\": {\n",
      "    \"train\": 0.7057255736710011,\n",
      "    \"test\": 0.7059297470937229\n",
      "  },\n",
      "  \"ACSMobility\": {\n",
      "    \"train\": 0.7359866494711265,\n",
      "    \"test\": 0.734137080422064\n",
      "  },\n",
      "  \"ACSEmployment\": {\n",
      "    \"train\": 0.5428142054730425,\n",
      "    \"test\": 0.5423693864853719\n",
      "  },\n",
      "  \"ACSTravelTime\": {\n",
      "    \"train\": 0.5696879960067078,\n",
      "    \"test\": 0.5708901510194466\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(all_tasks_max_const_acc, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714cd7d7",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
